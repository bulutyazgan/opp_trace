[
  {
    "fullName": "Bulut Yazgan",
    "linkedin_internal_id": "1034563486",
    "first_name": "Bulut",
    "last_name": "Yazgan",
    "public_identifier": "bulut-yazgan",
    "background_cover_image_url": "https://static.licdn.com/aero-v1/sc/h/5q92mjc5c51bjlwaj3rs9aa82",
    "profile_photo": "https://media.licdn.com/dms/image/v2/D4D03AQHnxFAb2Adg2Q/profile-displayphoto-scale_200_200/B4DZoDpKxiH4AY-/0/1760997716431?e=2147483647&v=beta&t=aGJg5az_obFciiXX8z8-RByHfOt5IELqdW0qyw0kZkQ",
    "headline": "1x Hackathon Winner | studying CS @ UCL",
    "location": "London, England, United Kingdom",
    "followers": "82 followers",
    "connections": "82 connections",
    "about": "Student in Computer Science Bsc, aspired AI/Cybersecurity Engineer",
    "experience": [
      {
        "position": "Malware Analyst",
        "company_url": "https://tr.linkedin.com/company/cyberwisecybersecurity?trk=public_profile_experience-item_profile-section-card_subtitle-click",
        "company_image": "https://media.licdn.com/dms/image/v2/D4D0BAQG2w_2uf-KmTA/company-logo_100_100/company-logo_100_100/0/1736519874235/cyberwisetr_logo?e=2147483647&v=beta&t=-rKi03d0a-ugxAfbMYAD4RA2JY1iqP7P_803qjg282E",
        "company_name": "Cyberwise",
        "location": "KadÄ±kÃ¶y, Ä°stanbul, TÃ¼rkiye",
        "summary": "",
        "starts_at": "Aug 2024",
        "ends_at": "Oct 2024",
        "duration": "3 months"
      },
      {
        "position": "Software Engineer Intern",
        "company_url": "https://tr.linkedin.com/company/alesta-ekspertiz?trk=public_profile_experience-item_profile-section-card_subtitle-click",
        "company_image": "https://media.licdn.com/dms/image/v2/D4D0BAQGKx_aia2zxjw/company-logo_100_100/company-logo_100_100/0/1692212180100/alesta_ekspertiz_logo?e=2147483647&v=beta&t=quv_ZGhW2bq50aQkZTLSTsnQvDCswJQNCwZ7crSriw4",
        "company_name": "Alesta UluslararasÄ± Ekspertiz / GÃ¶zetim & SÃ¶rvey Hizmetleri",
        "location": "Maltepe, Ä°stanbul, TÃ¼rkiye",
        "summary": "",
        "starts_at": "Aug 2024",
        "ends_at": "Sep 2024",
        "duration": "2 months"
      }
    ],
    "education": [
      {
        "college_url": "https://uk.linkedin.com/school/university-college-london/?trk=public_profile_school_profile-section-card_image-click",
        "college_name": "UCL",
        "college_image": "https://media.licdn.com/dms/image/v2/C4E0BAQEkPpD2CJSBpQ/company-logo_100_100/company-logo_100_100/0/1663656692926/university_college_london_logo?e=2147483647&v=beta&t=OYFYh5UTRQV6rA0LzeeoKxP9Gmx3RqmCayB9jS9moII",
        "college_degree": "Computer Science Bsc",
        "college_degree_field": "Computer Science",
        "college_duration": " - ",
        "college_activity": ""
      },
      {
        "college_url": "https://tr.linkedin.com/school/saint-beno%C3%AEt-frans%C4%B1z-lisesi/?trk=public_profile_school_profile-section-card_image-click",
        "college_name": "Ã–zel Saint BenoÃ®t FransÄ±z Lisesi / LycÃ©e FranÃ§ais Saint BenoÃ®t",
        "college_image": "https://media.licdn.com/dms/image/v2/C4D0BAQFNh68RSg5XeA/company-logo_100_100/company-logo_100_100/0/1678109226227/saint_benot_fransz_lisesi_logo?e=2147483647&v=beta&t=oxziqDrtn4bTb8RFpTxTlwwRbTJrplBRQR4ENJmB4TU",
        "college_degree": "",
        "college_degree_field": null,
        "college_duration": "2020 - 2025",
        "college_activity": ""
      }
    ],
    "articles": [],
    "description": {
      "description1": "Cyberwise",
      "description1_link": "https://tr.linkedin.com/company/cyberwisecybersecurity?trk=public_profile_topcard-current-company",
      "description2": "UCL",
      "description2_link": "https://uk.linkedin.com/school/university-college-london/?trk=public_profile_topcard-school",
      "description3": "Personal Website",
      "description3_link": "https://www.linkedin.com/redir/redirect?url=bulutyazgan%2Edev&urlhash=_2Vd&trk=public_profile_topcard-website"
    },
    "activities": [
      {
        "link": "https://www.linkedin.com/posts/suvan-goel-90a4b1304_48-hours-no-sleep-my-first-ever-hackathon-activity-7388990267699175424--Psq",
        "image": "https://media.licdn.com/dms/image/v2/D5622AQEyHt8bRr37Fg/feedshare-shrink_800/B56Zor3eNAIsAo-/0/1761672557901?e=2147483647&v=beta&t=fQ8ov5h-vCCL13KuiZPZ4YlgPeOBdHpfU4SwK-XtR48",
        "title": "48 hours. No sleep. My first ever hackathonâ€¦ and somehow we won.\n\nAs I strolled into Encode Club's building, I wasn't sure what to expect. A mix ofâ€¦",
        "activity": "Liked by Bulut Yazgan"
      },
      {
        "link": "https://www.linkedin.com/posts/borayazgan_what-a-great-europe-middle-east-gts-academy-activity-7382049366481571840-oQSP",
        "image": "https://media.licdn.com/dms/image/v2/D4D22AQHKyStWUlUk_w/feedshare-shrink_2048_1536/B4DZnJOxqlKsAw-/0/1760017718883?e=2147483647&v=beta&t=xwqvF9NRAoIbsKv-oYCRV-ujV8yMYEGZ1jRbPWjZ_w0",
        "title": "What a great Europe & Middle East GTS Academy!\nAn incredible two-day program filled with knowledge, networking, and inspiration.\nA huge thank youâ€¦",
        "activity": "Liked by Bulut Yazgan"
      },
      {
        "link": "https://www.linkedin.com/posts/le-nguyen-vu_power-of-friendship-pulls-through-it-was-activity-7386068709452226560-rG0V",
        "image": "https://static.licdn.com/aero-v1/sc/h/53n89ecoxpr1qrki1do3alazb",
        "title": "Power of friendship pulls through. It was fabulous working with Gabriel Gramicelli, Bulut Yazgan, and Guillermo Arevalo. ðŸ‘",
        "activity": "Liked by Bulut Yazgan"
      },
      {
        "link": "https://www.linkedin.com/posts/wyatt-ip_not-too-bad-for-my-first-ever-hackathon-activity-7386122105215680512-nyp2",
        "image": "https://media.licdn.com/dms/image/v2/D4E22AQFLFF4tAaZTuA/feedshare-shrink_1280/B4EZoDG6lRIQAs-/0/1760988736330?e=2147483647&v=beta&t=jnJRodVw_JypGQC5GFxGfLSGwqkpGO-aaNVJASy808E",
        "title": "Not too bad for my first ever hackathon!\n\nYesterday my team, the Wippy Wyverns, and I took 2nd place, out of 20 teams, in the DSS x AI x Lovable xâ€¦",
        "activity": "Liked by Bulut Yazgan"
      },
      {
        "link": "https://www.linkedin.com/posts/minh-dang-3194972b3_hackathon-innovation-productivity-activity-7385845520596107264-BC9-",
        "image": "https://media.licdn.com/dms/image/v2/D4E22AQG9wid1OR8rxg/feedshare-shrink_800/B4EZn_LXAgIUB8-/0/1760922793176?e=2147483647&v=beta&t=3cJvNOuYT4yQFi7Y1Q6PTnBJcb0srLVpknME81Hv5wA",
        "title": "What a Sunday! ðŸ¤©\n\nI just spent 5+ hours immersed in the \"vibecoding\" Hackathon \n-> This remind me one of the great lessons about innovation andâ€¦",
        "activity": "Liked by Bulut Yazgan"
      },
      {
        "link": "https://www.linkedin.com/posts/gabrielgramicelli_i-could-never-have-expected-this-heres-activity-7385917425537589248-ej-5",
        "image": "https://media.licdn.com/dms/image/v2/D4D22AQGn1wcX_OE61Q/feedshare-shrink_2048_1536/B4DZoAMwmDGgAw-/0/1760939936638?e=2147483647&v=beta&t=CEv203rAA1cif33Y9Ee3UXrdE0RpBl1XXNL5i019l0A",
        "title": "I could never have expected this. Hereâ€™s how I won 1st place at my 1st ever hackathon.\n\nLovable â¤ï¸ partnered up with the UCL DataSci and AI societiesâ€¦",
        "activity": "Liked by Bulut Yazgan"
      },
      {
        "link": "https://www.linkedin.com/posts/leochlon_thank-you-so-so-much-to-the-ucl-data-science-activity-7381062828486574080-Jnq5",
        "image": "https://media.licdn.com/dms/image/v2/D4E22AQHeV58GQEEr2A/feedshare-shrink_2048_1536/B4EZm7NgvUIMAw-/0/1759782510425?e=2147483647&v=beta&t=SXXDnAXItupcXaMlzM-hr4neaPg7IMjwCg96AMOsH6k",
        "title": "Thank you so so much to the UCL Data Science Society for inviting Hassana Labs  to talk to these actual geniuses. These talks are how I started myâ€¦",
        "activity": "Liked by Bulut Yazgan"
      },
      {
        "link": "https://www.linkedin.com/posts/mehmet-eren-alpan_segv0-activity-7367099426554687490-AJIp",
        "image": "https://media.licdn.com/dms/image/sync/v2/D4D27AQE4kbBeNEQMAg/articleshare-shrink_1280_800/B4DZj0wyIhGkAQ-/0/1756453086144?e=2147483647&v=beta&t=-ZOPSlHyS6h1U8y-KbCkFnQEj1BO-7FaeYe6IvmACZ8",
        "title": "Excited to launch segv0.pw â€” my new security blog. First posts cover a quick intro to Request Smuggling, a LIT-CTF writeup, the future of encryptionâ€¦",
        "activity": "Liked by Bulut Yazgan"
      },
      {
        "link": "https://tr.linkedin.com/posts/umut-santur-718742220_de%C4%9Ferli-ba%C4%9Flant%C4%B1lar%C4%B1m-herkes-staj%C4%B1n%C4%B1-bulmu%C5%9F-activity-7342117662442885120-zWbj",
        "image": "https://media.licdn.com/dms/image/v2/D4D22AQHJbCiiBGquHw/feedshare-shrink_800/B4DZeRxGkFGgAk-/0/1750497259240?e=2147483647&v=beta&t=I49zniTLqgT8uhzNTL8pPfX2v6xap6lNtszChO1PyQ4",
        "title": "DeÄŸerli baÄŸlantÄ±larÄ±m,\n\nHerkes stajÄ±nÄ± bulmuÅŸ, LinkedIn baÅŸarÄ± hikÃ¢yeleriyle dolup taÅŸarkenâ€¦\nBen de boÅŸ durmadÄ±m: CS2 bana 10 yÄ±llÄ±k tecrÃ¼be jetonumuâ€¦",
        "activity": "Liked by Bulut Yazgan"
      }
    ],
    "volunteering": [],
    "certification": [
      {
        "company_image": "https://static.licdn.com/aero-v1/sc/h/cs8pjfgyw96g44ln9r7tct85f",
        "certification": "Supervised Machine Learning: Regression and Classification",
        "company_url": "https://www.coursera.org/account/accomplishments/records/324X8T44HHZV?trk=public_profile_certification-title",
        "company_name": "DeepLearning.AI, Stanford University",
        "issue_date": "Issued Mar 2025",
        "credential_id": "Credential ID 324X8T44HHZV",
        "credential_url": "https://www.coursera.org/account/accomplishments/records/324X8T44HHZV?trk=public_profile_see-credential"
      }
    ],
    "people_also_viewed": [
      {
        "link": "https://it.linkedin.com/in/visionarynet?trk=public_profile_relatedPosts_face-pile-cta",
        "name": "",
        "summary": "Alessandro Ferrari\n        \n              \n          ðŸ‘½ðŸ‘½DAM for SAM2 Object TrackingðŸ‘½ðŸ‘½\n\nðŸ‘‰From the University of Ljubljana a novel distractor-aware drop-in memory module for SAM2. Reducing the tracking drift toward distractors and improves redetection capability after object occlusions. DAM4SAM outperforms SAM2.1, it's the new SOTA on 10 benchmarks. Repo released ðŸ’™\n\nð‡ð¢ð ð¡ð¥ð¢ð ð¡ð­ð¬:\nâœ…Novel drop-in distrator-aware memory (DAM)\nâœ…DAM4SAM: DAM into SAM2.1 model\nâœ…DAM into real-time SAM2- based model (TAM)\nâœ…DiDi Dataset: 180 clips, average length 1,500 frames\nâœ…New SOTA by a large margin on several benchmarks\n\n#artificialintelligence #machinelearning #ml #AI #deeplearning #computervision #AIwithPapers #metaverse #LLM\n\nðŸ‘‰Discussion https://lnkd.in/dMgakzWm\nðŸ‘‰Paper https://lnkd.in/dDPjRimk\nðŸ‘‰Project https://lnkd.in/d6vGU5Qx\nðŸ‘‰Repo https://lnkd.in/dSkZimVW",
        "location": "459\n              \n            \n      \n  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n            \n      \n        \n                8 Comments"
      },
      {
        "link": "https://in.linkedin.com/in/sreedath-panat?trk=public_profile_relatedPosts_face-pile-cta",
        "name": "",
        "summary": "Sreedath Panat\n        \n              \n          I recently implemented an intelligent surveillance system that uses YOLOv8 with pose estimation added on top of it. It was so much fun.\n\nHere is how I built it:\n\nThe architecture:\nStep 1: YOLOv8 nano [pretrained] for person detection\nI started with YOLOv8 nano as my backbone - it is incredibly lightweight but maintains good accuracy for person detection. The model runs at over 30 FPS on standard hardware, making it ideal for real-time applications. YOLOv8's improved anchor-free design gives cleaner bounding boxes, which is crucial for the next step.\n\nStep 2: Pose estimation layer\nFor each person detected by YOLO, I integrated a pose estimation model that extracts 17 key skeletal points. This runs only on the cropped person regions from YOLO's bounding boxes, making it computationally efficient.\n\nStep 3: Behavioral analysis engine\nThis is where I embedded a hand-engineered logic.\n-> Angle calculations between joints to detect aggressive stances\n-> Velocity tracking of key points to identify sudden movements\n-> Static pose analysis for loitering detection\n-> Ground-level pose patterns that might indicate someone concealing items\n\nUsed YOLOv8 nano specifically for its speed-accuracy trade-off\nImplemented a sliding window approach for temporal analysis\nAdded confidence thresholds to reduce false positives\nCreated custom alert triggers based on pose pattern combinations\n\nThe model is still not perfect. Crouching is often detected as lying down. But I guess it is not too difficult to make it better. I just wanted to share version 1 here.\n\nMy algorithm successfully flags unusual behaviors while maintaining low false positive rates. It can distinguish between normal activities like bending to pick something up versus suspicious crouching patterns.\n\nThe combination of YOLOv8's detection with pose-based analysis is a great system. Last week, two founders messaged me that they are using a system similar to this in their physical security product pipeline. So cool!\n\nI am taking a series of 2 lectures (3 hours+) on YOLO this week and next. If you are interested, you can register here for free as part of Vizuara Hands-on Computer Vision Bootcamp. The first lecture is at 2 pm IST today: https://lnkd.in/ghsVy7XF",
        "location": "291\n              \n            \n      \n  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n            \n      \n        \n                15 Comments"
      },
      {
        "link": "https://in.linkedin.com/in/kabilan-kb?trk=public_profile_relatedPosts_face-pile-cta",
        "name": "",
        "summary": "Kabilan Kb\n        \n              \n          Excited to share my latest demo!\nI integrated XLeRobot inside NVIDIA Isaac Sim within a kitchen environment created by Mustafa Mohammadi from Lightwheel ðŸ ðŸ³. The robot was not only simulated but also operated using a real robot arm â€” enabling limitation learning by bridging the gap between simulation and hardware.\nThis workflow shows how we can:\n ðŸ”¹ Prototype in rich virtual environments (like kitchens for household robotics).\n ðŸ”¹ Transfer skills directly from simulation to the real world.\n ðŸ”¹ Explore practical use-cases of affordable platforms like XLeRobot for embodied AI.\nâœ¨ Whatâ€™s next?\n Iâ€™ll be sharing an upcoming blog and a YouTube video tutorial on this workflow â€” diving deeper into:\n âœ”ï¸ Reinforcement learning for skill transfer\n âœ”ï¸ VR teleoperation with AI models\n âœ”ï¸ Integrating Groot N1.5 for advanced control\nThe combination of Isaac Sim + XLeRobot + real-world control opens doors for research in household assistance, human-robot interaction, and teaching robots everyday tasks.\nNVIDIA Robotics\nNinad Madhab Dustin Franklin Jigar Halani Edmar Mendizabal Pradeep Kulasekaran \n#IsaacSim #XLeRobot #EmbodiedAI #Robotics #SimulationToReality #HouseholdRobotics #AI #VR",
        "location": "306\n              \n            \n      \n  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n            \n      \n        \n                12 Comments"
      },
      {
        "link": "https://es.linkedin.com/in/ahcorde/en?trk=public_profile_relatedPosts_face-pile-cta",
        "name": "",
        "summary": "Alejandro HernÃ¡ndez Cordero\n        \n              \n          A Motion Planning Library Featuring the FMTX Algorithm [1]\n\nThis repository contains a C++ library for advanced motion planning. FMTX is a sampling-based replanning method, derived from FMT* [2], designed for efficient navigation in dynamic environments.\n\nAdvanced Data Structures:\n - NanoFLANN: For fast nearest-neighbor searches.\n - Weighted NanoFLANN: To prioritize certain state-space dimensions.\nROS 2 and Gazebo Integration:\n - integrated with ROS 2 for visualization (RViz) and communication and collision detection.\n - Modular and Extensible:\n\n#ros #ros2 #gazebo #simulation #opensource #navigation #mobilerobot #robot #robotics #motionplanning #replanning\n\n[1] https://lnkd.in/dmrtwczb\n[2] https://lnkd.in/dZFG5Jra",
        "location": "337\n              \n            \n      \n  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n            \n      \n        \n                1 Comment"
      },
      {
        "link": "https://dk.linkedin.com/in/nicolaiai?trk=public_profile_relatedPosts_face-pile-cta",
        "name": "",
        "summary": "Nicolai Nielsen\n        \n              \n          Multi-View 3D Point Tracking ðŸ–¼ï¸ðŸ”¥ Track arbitrary points in dynamic scenes using multiple camera views\n\n\nMVTracker is the first data-driven multi-view 3D point tracker designed to track arbitrary points in dynamic scenes using multiple camera views. Unlike monocular trackers, which struggle with depth ambiguities and occlusion, or previous multi-camera methods that require over 20 cameras and tedious per-sequence optimization, the MVTracker feed-forward model directly predicts 3D correspondences using a practical number of cameras, in this case 4, enabling robust and accurate online tracking.\n\n\nGiven multi-view RGB videos and camera parameters the method first extracts per-view feature maps using a CNN encoder. A fused 3D point cloud is the constructed from estimated or sensor-provided depth, associating each point with learned features. Directed kNN-based correlation links points across space and time, capturing spatiotemporal relationships across views. A transformer iteratively refines point trajectories using attention over multi-view correlations. The model processes sequences in overlapping sliding windows, producing temporally consistent 3D point trajectories with occlusion-aware visibility predictions.\n\nðŸ‘‰ Check out the paper, examples and code here: https://lnkd.in/dB-adJwy",
        "location": "196\n              \n            \n      \n  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n            \n      \n        \n                5 Comments"
      },
      {
        "link": "https://il.linkedin.com/in/ziv-meri?trk=public_profile_relatedPosts_face-pile-cta",
        "name": "",
        "summary": "Ziv Meri\n        \n              \n          The bar for robotics engineers keeps getting raised.\n\nToday, they're expected to master:\n\nâ†’ Perception: \nDepth sensing, 3D vision, object detection, feature matching. \n\nâ†’ Kinematics & dynamics: \nMechanics, rigid body transformations, numerical ODE solvers. \n\nâ†’ State estimation: \nBayes, particle, and Kalman filters.   \n\nâ†’ Path planning: \nPure pursuit, dynamic programming, reinforcement learning. \n\nâ†’ Motion control: \nPID and model predictive control. \n\nThat's before the \"standard\" stack: optimization, programming, toolboxes, simulations...\n\nThe good news?\n\nNo other engineering field is as rewarding. \nRobotics, autonomous vehicles, and GNC give you the chance to solve real-world problems and build systems that influence millions of lives.\n\nWith the right guidance, you can turn these challenges into skills that open doors to great opportunities.",
        "location": "477\n              \n            \n      \n  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n            \n      \n        \n                25 Comments"
      },
      {
        "link": "https://br.linkedin.com/in/yan-barros-yan?trk=public_profile_relatedPosts_face-pile-cta",
        "name": "",
        "summary": "Yan Barros\n        \n              \n          ðŸš€ Open Source Release: PINNFactory\n\nAfter seeing the amazing engagement from the community around Physics-Informed Neural Networks (PINNs), I decided to release PINNFactory as an open source project! ðŸŒŸ\n\nPINNFactory is a lightweight Python framework for building PINNs from symbolic equations, combining SymPy and PyTorch to enable:\n\n- Flexible neural network architectures\n- Inverse parameter estimation\n- Automatic generation of loss functions from PDEs and conditions\n\nThe goal? Build together with the community. ðŸ’¡\n\nWhether you're a researcher, engineer, or AI enthusiast working with physics-based problems, now is your chance to contribute, suggest improvements, open issues, or submit code.\n\nðŸ“‚ Check out the project: https://lnkd.in/djr9khFA\n\nLet's accelerate the evolution of PINNs and create something meaningful together! âš¡\n\n#OpenSource #PINNs #MachineLearning #DeepLearning #Python #Collaboration #ScientificComputing",
        "location": "602\n              \n            \n      \n  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n            \n      \n        \n                26 Comments"
      },
      {
        "link": "https://be.linkedin.com/in/enzo-ghisoni-robotics?trk=public_profile_relatedPosts_face-pile-cta",
        "name": "",
        "summary": "Enzo Ghisoni\n        \n              \n          Exploration in ROS 2: Make your Robot map the unknown\n\nIf you are interested about autonomous mapping robotics, you should definitely give a try to explore_lite. The aim of the package is to explore unknown environments to build your maps. RafaÅ‚ GÃ³recki from Husarion has written a pretty good tutorial to setup and use explore_lite on your ROS 2 Robots with Nav2 and slam_toolbox.\n\nUsing the explore_lite package, your robot can:\n âœ… Automatically pick unexplored frontiers on a map\n âœ… Navigate towards them using Nav2\n âœ… Expand its knowledge with slam_toolbox\n âœ… Repeat until the whole area is discovered\n\nThis complete the cycle from SLAM â†’ Path Planning â†’ Exploration, showing how a ROSbot (or its Gazebo simulation) can explore and map an environment with minimal human input. Itâ€™s a great example of how ROS 2 tools come together to turn theory into practice.\n\nðŸ”— Check the tutorial and add explore_lite in your autonomous toolkit!\n\nWhich ROS 2 packages are you using for autonomous exploration?\n\nLet's connect and share robotics tips ðŸ”½ \n#Robotics #ROS2 #SLAM",
        "location": "238\n              \n            \n      \n  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n            \n      \n        \n                16 Comments"
      },
      {
        "link": "https://in.linkedin.com/in/jino-rohit-6032541b5?trk=public_profile_relatedPosts_face-pile-cta",
        "name": "",
        "summary": "Jino Rohit\n        \n              \n          I'm building my own OpenCV from scratch - fastcv. \n\nfastcv is a C++ CUDA rewrite with Pytorch bindings of the image filters in the OpenCV library.\n\nThis time I come with more benchmarks(RTX 4060 Ti)\n1. fastcv sobel edge detector kernel is ðŸ­ðŸ®ðŸ¬ðŸ¬ð˜… faster than opencv for 4k images.\n2. fastcv blur kernel is ðŸ°ð˜… faster than opencv for 4k images.\n3. fastcv grayscale kernel is ðŸ­ðŸ®ð˜… faster than opencv for 4k images.\n\nWriting more kernels every single day!\n\nRepo - https://lnkd.in/gamqzr-W",
        "location": "742\n              \n            \n      \n  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n            \n      \n        \n                29 Comments"
      },
      {
        "link": "https://be.linkedin.com/in/antoine-vm?trk=public_profile_relatedPosts_face-pile-cta",
        "name": "",
        "summary": "Antoine Van Malleghem\n        \n              \n          ROS 2 Tutorials for Beginners by DigiKey\n\nIf youâ€™re just getting started with ROS 2, DigiKey has launched an amazing 12-part beginner-friendly video series!\n\nThey just released the third episode, with one new episode coming out each week. The course is perfect for newcomers: you only need basic Python or C++ knowledge, no prior ROS experience required.\n\nðŸŽ¥ Start here: https://lnkd.in/eeR5AzvU\n\nEach tutorial comes with:\n* A dedicated GitHub repo full of resources\n* Written explanations on their website\n* Links to all hardware components used\n\nðŸ’¡ In my opinion, itâ€™s one of the best ways to learn ROS hands-on: on a real robot project. If you already have a Raspberry Pi, the robot kit costs around $120.\n\nWhether youâ€™re a student, hobbyist, or engineer looking to dive into robotics, this is a fantastic place to start!\n\n#ROS2",
        "location": "112\n              \n            \n      \n  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n            \n      \n        \n                5 Comments"
      },
      {
        "link": "https://www.linkedin.com/in/daviddorf?trk=public_profile_relatedPosts_face-pile-cta",
        "name": "",
        "summary": "David Dorf\n        \n              \n          Quick update on pyppet, it can be converted to URDF to be used with ROS now! I've added several other features you can check out under the newest release tag. You can check out the latest pyppet [v1.0.2] and pyppet-models [v1.1.1] on either PyPI or GitHub.\n\nhttps://lnkd.in/eyjbnPUW\nhttps://lnkd.in/ew4yGmaC",
        "location": "42\n              \n            \n      \n  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n            \n      \n        \n                3 Comments"
      },
      {
        "link": "https://ug.linkedin.com/in/tafar-m-b46337259?trk=public_profile_relatedPosts_face-pile-cta",
        "name": "",
        "summary": "Tafar M.\n        \n              \n          In autonomous driving, accuracy is just the beginning. The real challenge is building systems we can trust. That's why I prioritized transparency in my latest project.\n\nI've just completed an autonomous driving simulation system that achieves 95%+ detection accuracy by pairing ð˜ðŽð‹ðŽð¯ðŸ– with classical computer vision algorithms for lane detection.\n\nð–ð¡ð² ð­ð¡ðž ð¡ð²ð›ð«ð¢ð ðšð©ð©ð«ð¨ðšðœð¡? While neural networks are powerful, deterministic algorithms provide something essential.\nEvery â€¢ turn, â€¢ stop and â€¢ acceleration in my system is governed by verifiable logic, not just a confidence score.\n\nâœ“ Key performance benchmarks:\nâ†’ Real-time: >30 FPS with <50ms latency.\nâ†’ Traceable: Every decision is fully explainable.\nâ†’ Modular: Designed for easy iteration and real-world testing.\n\nThis project proves we can build high-performance autonomous systems without sacrificing transparency.\n\nA huge thank you to Ultralytics YOLOv8â€™s balance of speed and accuracy was the critical component that made this possible.\n\nðŸ”“The full codebase is open-sourced on my GitHub: https://lnkd.in/d9qgn8de\n\nIf you're working on safety-critical AI, I encourage you to explore this hybrid methodology. Let me know what you think ðŸ‘‡\n\n#AutonomousDriving #ComputerVision #AI #MachineLearning #YOLOv8 #Ultralytics #Python #OpenCV #OpenSource #DeepLearning",
        "location": "162"
      },
      {
        "link": "https://be.linkedin.com/in/mohammed-el-amine-mokhtari?trk=public_profile_relatedPosts_face-pile-cta",
        "name": "",
        "summary": "Mohammed El Amine Mokhtari\n        \n              \n          The best decision we made when building our DICOM viewer was to convert each functionality into a plugin (or module/extension, we use the term plugin at PYCAD).\n\nThe advantage of this approach is that it gives you a solid skeleton for your web application, which can support different layout types such as MPR, MPR+VR, or MPR+3D Mesh. Then, you can create separate modules that use VTK in the backend and Vue.js in the frontend, and integrate them all into a Trame application.\n\nThe idea behind plugins is that you can build multiple modules and simply plug or unplug them based on client requirements. This also makes it easy to create custom plugins for a specific client without touching the core app or other plugins.\n\nAll you need are the connections to the core app; meaning how to receive inputs and send outputs. And your plugin will fit seamlessly into the viewer without interfering with the other plugins.\n\nThis isnâ€™t a new concept; in fact, itâ€™s the same idea used in 3D Slicer, where they call them extensions. Whenever you want to create a new extension in 3D Slicer, thereâ€™s a specific template you need to follow, which ensures your module integrates smoothly and interacts with all base modules without requiring changes to the main viewer.\n\nI wanted to share this tip with you. You might find it helpful when building your own DICOM viewer. And if you ever need help, you can always reach out to the PYCAD team.",
        "location": "95\n              \n            \n      \n  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n            \n      \n        \n                6 Comments"
      },
      {
        "link": "https://uk.linkedin.com/in/nelson-elijah?trk=public_profile_relatedPosts_face-pile-cta",
        "name": "",
        "summary": "Nelson Elijah\n        \n              \n          ðŸš€ Excited to share that my research, â€œAdvancing Slip Classification in Robotic Manipulation Through Tactile Data Representation and Model Selection,â€ has been published in the IEEE Sensors Journal (Q1)!\n\nThe Challenge: In robotic manipulation, slip (when an object begins to move undesirably in a robotâ€™s grasp) is one of the main causes of failure. Detecting and predicting slips in real-time is critical for achieving reliable, human-like dexterity, yet existing approaches often struggle in the presence of imbalanced data and complex tactile signals.\n\nOur approach: We evaluated tactile data representations from 1D to 4D with CNNs, RNNs, LSTMs, and Transformers. Models that learned the spatial and temporal structure of taxels delivered the strongest results, and our new SMOTE-CNN reduced dataset imbalance to boost slip detection accuracy.\n\nKey Results:\nðŸ“Š +3% F1-score improvement over baselines\nðŸ›¡ï¸ 95% reduction in false negatives (crucial for avoiding dropped objects)\nðŸ¤– Real-time slip avoidance with Model Predictive Control (MPC)\nðŸ“‚ Open-source tactile dataset for benchmarking future research\n\nWhy It Matters: Robots that can reliably sense, predict, and prevent slips will be far more capable in real-world environmentsâ€”from manufacturing and logistics to healthcare and assistive robotics.\n\nA huge thank you to:\nðŸ™ Prof. Amir Ghalamzan for giving me the chance and resources to apply my skills in ML and sensing systems at the Intelligent Manipulation Lab (Lincoln Centre for Autonomous Systems (L-CAS)), University of Lincoln.\nðŸ™ Mojtaba Esfandiari for his thorough and compassionate feedback.\nðŸ™ Kiyanoush Nazari for his patient mentorship. Kiyanoush taught me the ropes of research, from how to read daunting papers and present research rigorously to facing reviewer criticism with courage. His words, â€œIf your research took a huge effort to produce, then your presentation must take twice as much,â€ live rent-free in my soul.\n\nðŸ“„ Read our paper here: https://lnkd.in/d9c6Sfd5\n\nThis is one step closer to building robots with the dexterity and adaptability needed for the next generation of intelligent manipulation. Also, thanks to Oluwatimilehin Owolabi for standing by me during the crazy hours of this work.\n\n#Robotics #TactileSensing #DeepLearning #Manipulation #IEEE #ML",
        "location": "323\n              \n            \n      \n  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n            \n      \n        \n                93 Comments"
      }
    ],
    "similar_profiles": [
      {
        "link": "https://tr.linkedin.com/in/bulut-yazgan-179710ab?trk=public_profile_samename-profile",
        "name": "bulut yazgan",
        "summary": "--",
        "location": "TÃ¼rkiye"
      },
      {
        "link": "https://tr.linkedin.com/in/bulut-yazgan-601519240?trk=public_profile_samename-profile",
        "name": "Bulut Yazgan",
        "summary": "Ã–zel Saint BenoÃ®t FransÄ±z Lisesi / LycÃ©e FranÃ§ais Saint BenoÃ®t eÄŸitim kurumunda Ã¶ÄŸrenci",
        "location": "Istanbul"
      }
    ],
    "recommendations": [],
    "publications": [],
    "courses": [],
    "languages": [],
    "organizations": [],
    "projects": [],
    "awards": [],
    "score": []
  }
]

